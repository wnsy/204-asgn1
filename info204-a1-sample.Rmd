---
title: "INFO 204 Assignment 1 Template - Data Science Job Locations"
date: "18 August 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<span style="color: #ce2227">
**_If you have worked on this assignment in groups, then only a single submission is required from the group, but you should make a note of collaborators at the start of the document_**
</span>

# Group Members
- Ayesha Wan Ismail
- Bhavisha Solanki
- Ruth Tobile Banda

This assignment template should be considered a rough guide as an acceptable type of project for your first INFO 204 assignment. You can deviate from the actual process (e.g., scrape pages from two different web sites), but the main points to focus on are that:

  1. you provide a clear description of your problem that can be followed without any specific code; and
  2. your code matches the description of your workflow.
  
Your "analysis" following the data scraping does not need to be comprehensive (see below for how simple it may be). However, if you perform a more substantial analysis of your results, then this will be taken into account should you have missed anything earlier in your assignment and marks that you may have lost in earlier sections may be recovered.

# Problem Definition

Finding the most in-demand jobs around the world right now to see which countries/continents lack in which set of skill and also use this as a general guideline template for future students to look through in order to make relevant and practical study choices. Other question that we can ask would be is there a vast difference between countries and if there is a bias between the choices made by the population in these countries? 


To answer this question, we could follow this manual process:
  1. Visit the [Western Union] website and go to a blog article "[The Most In-Demand Careers Around the World]". 
  2. Underneath the main title, within sub-headings, we would see a list of jobs in-demand and the countries which are in short of them:
    a. Identify the jobs list (e.g. Teachers, Accountants) headings and countries in demand.
    b. Populate it into a vector.
    c. Add the identified location information to our list of results
 

  1. Visit the [Seek] web site and enter a search query for "data science".
  2. On the [query results] page, we'd see a list of "featured" jobs and "normal" vacancies. For each "normal" job, we could then:
    a. Follow the link provided by the vacancy advertisement to get the job information page
    b. Identify the relevant "Location" information on the page
    c. Add the identified location information to our list of results
  3. Once we've visited all the relevant vacancy ads, we have a list of locations that we could tabulate - job done!
  
## Required Components

The [Seek] site has a search query interface that allows us to search for jobs via keywords:

![The Seek website (compact interface shown) with "data science" keywords ready to be searched](images/seek.png)

The way that Seek returns its search results is through a page with well-defined query parameters: https://www.seek.co.nz/jobs?keywords=%22data+science%22, which leads to the following page:

![The Seek search results page for "data science" jobs (compact interface shown)](images/searchresults.png)

The search results are prefixed with "promoted" jobs (which are repeated later under the "normal" job section) that we need to eliminate from our scraping. Fortunately, the normal job results can be easily filtered (in this case, using XPath). Then within each job advertisement, we want to extract the title element of the job (as this contains the link to the job description), so that we can then jump to this page and extract the details that we need:

![The parts of the search results that we need to scrape (compact interface shown)](images/highlighted.png)

So, once we are on the search results page, we can use the developer tools to explore the source behind the page, and from this, we can see that we need the article elements that have a data-automation attribute setting of 'normalJob':
![The parts of the job details that we need to scrape (compact interface shown)](images/xpath-selector-job.png)
Therefore, we can scrape the required job information with the XPath selector:
`"//article[@data-automation='normalJob']"`
Note that we haven't really talked about XPath in INFO 204 - it is an alternative to using CSS selectors that provide more fined-grained control over the access (but the syntax can sometimes be a little bit confusing, so use it only when absolutely necessary!). More details on XPath can be found [at this tutorial](https://www.w3schools.com/xml/xml_xpath.asp).

Similarly, we can use the developer tools to identify what part of these article elements contains the title and hyperlink. A little bit of drilling tells us that the title is an h1 (heading type 1) element, and there in an a (anchor) element under this with the link that we need. Therefore, once we have each of the article elements pertaining to the jobs, we can access the anchor element using the CSS selector:
`"h1 a"`, and then access the href attribute from this element to get to the next page.

The next page has a fairly simple structure that should be easily scraped:

![The parts of the dearch results that we need to scrape (compact interface shown)](images/dev-tools-jobdetails.png)
Once the required element is identified, we can use the developer tools in Chrome to copy the required CSS selector, which is the awkward:
`"#app > div > div > div:nth-child(3) > div > div > div:nth-child(3) > div > div.PxPTnGA > div > article > div > div:nth-child(1) > span > div > section > dl > dd:nth-child(4) > span > span > strong"`.
With a bit of tweaking, we can make this selector somewhat less awkward:
`"dl > dd:nth-child(4) > span > span > strong"`.

With the XPath/CSS selectors identified, and a general workflow in place, we can now proceed with the scraping.

# Scraping Procedure
We start with defining a few key components (the required libraries, starting url, and key XPath/CSS selectors):
```{r scraping setup, echo=TRUE}
library(rvest)

search.url <- "https://www.seek.co.nz/jobs?keywords=%22data+science%22"
job.selector <- "//article[@data-automation='normalJob']" ## note: XPath selector!
title.selector <- "h1 a"
```

As, we're going to be scraping multiple pages, and the source links will be scraped from our start document, we will need to use a session to keep track of the relevant details:
```{r session, echo=TRUE}
doc <- html_session(search.url)
```

And now, we should retrieve our required list of advertised jobs:
```{r fetch jobs, echo=TRUE}
jobs <- html_nodes(doc, xpath=job.selector)
cat("Fetched", length(jobs), "results\n")
```
note that we used the `xpath` parameter of the `html_nodes()` function instead of using a normal CSS selector. This was due to the selection of by attribute status, which cannot be easily done using a CSS selector.

Having found the `r length(jobs)` jobs, we need to extract the required hyperlinks that lead us to the job description pages:
```{r fetch urls, echo=TRUE}
job.links <- html_nodes(jobs, title.selector)
job.href <- html_attr(job.links, "href")
```

Now, we perform the iteration over the href attributes that we discovered, extract the relevant page elements, and add them into a results list (steps 2 a, b, and c in our previously defined workflow):
```{r scraper iteration, echo=TRUE}
location.selector <- "dl > dd:nth-child(4) > span > span > strong"
job.locations <- NULL ## a container for our results, starts off empty
for (job in job.href) {
  job.loc <- tryCatch({
    job.doc <- jump_to(doc, job)
    job.loc <- html_node(job.doc, location.selector)
    
    html_text(job.loc)
  }, error=function(e) NULL)

  ## add the next location to our results vector
  job.locations <- c(job.locations, job.loc)
}
job.locations
```
Note the use of the `tryCatch()` function here - this is to handle any cases where following a link may produce an error (e.g., a 404 error for a broken link referring to a missing page).

# Analysis
Our `job.locations` vector is now complete, and should contain an entry for every job advertised. Now, we can create a tally of these locations and plot them using a suitable method (e.g., a bar plot):
```{r summary, echo=TRUE, fig.align="center", out.width="100%"}
tally <- table(job.locations)
barplot(tally, main="Location of Data Science Jobs on Seek", ylab="# Jobs Found", col="#00508f")
```

The analysis done here suggests that the majority of data science jobs are in the North Island and centred around either Auckland or Wellington.

## Assumptions
The analysis here is rather simple - we have only really considered a single search for jobs on a single web site. To be more rigorous, we should examine multiple web sites, and maybe attempt to consider other terms often confused with or related to "data science" (e.g., "predictive analytics", "data mining", "machine learning").

[Seek]: https://seek.co.nz
[query results]: https://www.seek.co.nz/jobs?keywords=%22data+science%22
[Western Union]: https://www.westernunion.com/
[The Most In-Demand Careers Around the World]: https://www.westernunion.com/blog/jobs-in-demand/


